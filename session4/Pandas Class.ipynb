{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "842079f8",
   "metadata": {},
   "source": [
    "# Manually creating a Dataframe or Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d3fe0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92542f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Manually creating a series\n",
    "students = ['Alice', 'Jack', 'Molly']\n",
    "\n",
    "# Now we just call the Series function in pandas and pass in the students\n",
    "pd.Series(students)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cca083a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "numbers = [1, 2, 3]\n",
    "# And turn that into a series\n",
    "pd.Series(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20334ac0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#creating series with a specific index\n",
    "s = pd.Series(['Physics', 'Chemistry', 'English'], index=['Alice', 'Jack', 'Molly'])\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117cb268",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creating a series with specific index but using a dictionary\n",
    "students_scores = {'Alice': 'Physics',\n",
    "                   'Jack': 'Chemistry',\n",
    "                   'Molly': 'English'}\n",
    "# When I create the series object though I'll only ask for an index with three students, and\n",
    "# excluding Jack\n",
    "s = pd.Series(students_scores, index=['Alice', 'Molly', 'Sam'])\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cc4754",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2c9441",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#One could use loc definitions to access the data\n",
    "print(s.loc['Molly'],s['Molly'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054bd6c5",
   "metadata": {},
   "source": [
    "## Concatenation and non-unique indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bb0432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Up until now I've shown only examples of a series where the index values were unique. I want \n",
    "# to end this section by showing an example where index values are not unique, and this makes \n",
    "# pandas Series a little different conceptually then, for instance, a relational database.\n",
    "\n",
    "# Lets create a Series with students and the courses which they have taken\n",
    "students_classes = pd.Series({'Alice': 'Physics',\n",
    "                   'Jack': 'Chemistry',\n",
    "                   'Molly': 'English',\n",
    "                   'Sam': 'History'})\n",
    "students_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33680a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets create a Series just for some new student Kelly, which lists all of the courses\n",
    "# she has taken. We'll set the index to Kelly, and the data to be the names of courses.\n",
    "kelly_classes = pd.Series(['Philosophy', 'Arts', 'Math'], index=['Kelly', 'Kelly', 'Kelly'])\n",
    "kelly_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0ef96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we can append all of the data in this new Series to the first using the .append()\n",
    "# function.\n",
    "all_students_classes = pd.concat([students_classes,kelly_classes])\n",
    "\n",
    "# This creates a series which has our original people in it as well as all of Kelly's courses\n",
    "all_students_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3910239c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_students_classes['Kelly']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f5488a-503b-4398-8322-1cfee196a774",
   "metadata": {},
   "source": [
    "## Question: Why is it important to be aware of non-unique indexes when concatenating DataFrames? What issues might arise if indexes are not unique?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923d9b63",
   "metadata": {},
   "source": [
    "# Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dded3eb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "students = [{'Name': 'Alice',\n",
    "              'Class': 'Physics',\n",
    "              'Score': 85},\n",
    "            {'Name': 'Jack',\n",
    "             'Class': 'Chemistry',\n",
    "             'Score': 82},\n",
    "            {'Name': 'Helen',\n",
    "             'Class': 'Biology',\n",
    "             'Score': 90}]\n",
    "\n",
    "# Then we pass this list of dictionaries into the DataFrame function\n",
    "df = pd.DataFrame(students, index=['school1', 'school2', 'school1'])\n",
    "# And lets print the head again\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd035f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Similar to the series, we can extract data using the .iloc and .loc attributes. Because the \n",
    "# DataFrame is two-dimensional, passing a single value to the loc indexing operator will return \n",
    "# the series if there's only one row to return.\n",
    "\n",
    "# For instance, if we wanted to select data associated with school2, we would just query the \n",
    "# .loc attribute with one parameter.\n",
    "df.loc['school2']\n",
    "\n",
    "# You'll note that the name of the series is returned as the index value, while the column \n",
    "# name is included in the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfa93e9-7214-427a-a896-a1cf6d20ea15",
   "metadata": {},
   "source": [
    "## Common misconceptions, question time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b714a4-9525-462d-b540-18dbae188e11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Why does the following command fails?\n",
    "#df['school2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39a649f-cbb5-4260-bac6-b4dd7c01599a",
   "metadata": {},
   "source": [
    "## Question: What are the differences between a Pandas Series and a DataFrame? Can you give an example of when you might use each?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efeec6a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We can check the data type of the return using the python type function.\n",
    "type(df.loc['school2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1871144d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# It's important to remember that the indices and column names along either axes horizontal or \n",
    "# vertical, could be non-unique. In this example, we see two records for school1 as different rows.\n",
    "# If we use a single value with the DataFrame lock attribute, multiple rows of the DataFrame will \n",
    "# return, not as a new series, but as a new DataFrame.\n",
    "\n",
    "# Lets query for school1 records\n",
    "df.loc['school1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aa6763",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# One of the powers of the Panda's DataFrame is that you can quickly select data based on multiple axes.\n",
    "# For instance, if you wanted to just list the student names for school1, you would supply two \n",
    "# parameters to .loc, one being the row index and the other being the column name.\n",
    "\n",
    "# For instance, if we are only interested in school1's student names\n",
    "# Important to note, it only works with different axes, i.e., an index (one type of the rows) and a column\n",
    "\n",
    "df.loc['school1', 'Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70742af3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remember, just like the Series, the pandas developers have implemented this using the indexing\n",
    "# operator and not as parameters to a function.\n",
    "\n",
    "# What would we do if we just wanted to select a single column though? Well, there are a few\n",
    "# mechanisms. Firstly, we could transpose the matrix. This pivots all of the rows into columns\n",
    "# and all of the columns into rows, and is done with the T attribute\n",
    "df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ce8089",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Then we can call .loc on the transpose to get the student names only\n",
    "df.T.loc['Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c62d1da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# However, since iloc and loc are used for row selection, Panda reserves the indexing operator \n",
    "# directly on the DataFrame for column selection. In a Panda's DataFrame, columns always have a name. \n",
    "# So this selection is always label based, and is not as confusing as it was when using the square \n",
    "# bracket operator on the series objects. For those familiar with relational databases, this operator \n",
    "# is analogous to column projection.\n",
    "df['Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416e7bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can select multiple columns if you put them in a list.\n",
    "df[['Name','Class']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd827b1b",
   "metadata": {},
   "source": [
    "## Interacting with dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe882333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we leave the discussion of accessing data in DataFrames, lets talk about dropping data.\n",
    "# It's easy to delete data in Series and DataFrames, and we can use the drop function to do so. \n",
    "# This function takes a single parameter, which is the index or row label, to drop. This is another \n",
    "# tricky place for new users -- the drop function doesn't change the DataFrame by default! Instead,\n",
    "# the drop function returns to you a copy of the DataFrame with the given rows removed.\n",
    "\n",
    "df.drop('school1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd30e92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# But if we look at our original DataFrame we see the data is still intact.\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c7e5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop has two interesting optional parameters. The first is called inplace, and if it's \n",
    "# set to true, the DataFrame will be updated in place, instead of a copy being returned. \n",
    "# The second parameter is the axes, which should be dropped. By default, this value is 0, \n",
    "# indicating the row axis. But you could change it to 1 if you want to drop a column.\n",
    "\n",
    "# For example, lets make a copy of a DataFrame using .copy()\n",
    "copy_df = df.copy()\n",
    "# Now lets drop the name column in this copy\n",
    "copy_df.drop(\"Name\", inplace=True, axis=1)\n",
    "copy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937bb48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a second way to drop a column, and that's directly through the use of the indexing \n",
    "# operator, using the del keyword. This way of dropping data, however, takes immediate effect \n",
    "# on the DataFrame and does not return a view.\n",
    "del copy_df['Class']\n",
    "copy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13b54fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, adding a new column to the DataFrame is as easy as assigning it to some value using\n",
    "# the indexing operator. For instance, if we wanted to add a class ranking column with default \n",
    "# value of None, we could do so by using the assignment operator after the square brackets.\n",
    "# This broadcasts the default value to the new column immediately.\n",
    "\n",
    "df['ClassRanking'] = None\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bf498c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pandas mades it easy to turn a CSV into a dataframe, we just call read_csv()\n",
    "#df = pd.read_csv('Admission_Predict.csv')\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/calovids/IMPRS-Python-Workshop/main/data/Admission_Predict.csv')\n",
    "\n",
    "# And let's look at the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748f8b74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We notice that by default index starts with 0 while the students' serial number starts from 1. If you jump\n",
    "# back to the CSV output you'll deduce that pandas has create a new index. Instead, we can set the serial no.\n",
    "# as the index if we want to by using the index_col.\n",
    "#df = pd.read_csv('Admission_Predict.csv', index_col=0)\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/calovids/IMPRS-Python-Workshop/main/data/Admission_Predict.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c664a6-ff98-4b9f-bbad-342c8b4730ca",
   "metadata": {},
   "source": [
    "### Renaming variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1755cd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Notice that we have two columns \"SOP\" and \"LOR\" and probably not everyone knows what they mean So let's\n",
    "# change our column names to make it more clear. In Pandas, we can use the rename() function It takes a\n",
    "# parameter called columns, and we need to pass into a dictionary which the keys are the old column name and\n",
    "# the value is the corresponding new column name\n",
    "new_df=df.rename(columns={'SOP': 'Statement of Purpose',\n",
    "                          'LOR': 'Letter of Recommendation'})\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9c6ef3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Note that it didn't seem to change the LOR variable, let's have a look why:\n",
    "new_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c39ac53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We can create some function that does the cleaning of variables and then tell renamed to apply that function\n",
    "# across all of the data. Python comes with a handy string function to strip white space called \"strip()\".\n",
    "# When we pass this in to rename we pass the function as the mapper parameter, and then indicate whether the\n",
    "# axis should be columns or index (row labels)\n",
    "new_df=new_df.rename(mapper=str.strip, axis='columns')\n",
    "# Let's take a look at results\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6b9424",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_df=new_df.rename(columns={'SOP': 'Statement of Purpose',\n",
    "                              'LOR': 'Letter of Recommendation'})\n",
    "new_df.head()\n",
    "#let's save this for later:\n",
    "AdmissionDF=new_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6099879c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# One nice feature of Pandas is multi-level indexing. This is similar to composite keys in \n",
    "# relational database systems. To create a multi-level index, we simply call set index and \n",
    "# give it a list of columns that we're interested in promoting to an index.\n",
    "\n",
    "# Pandas will search through these in order, finding the distinct data and form composite indices.\n",
    "# A good example of this is often found when dealing with geographical data which is sorted by \n",
    "# regions or demographics.\n",
    "\n",
    "# Let's change data sets and look at some census data for a better example. This data is stored in \n",
    "# the file census.csv and comes from the United States Census Bureau. In particular, this is a \n",
    "# breakdown of the population level data at the US county level. It's a great example of how \n",
    "# different kinds of data sets might be formatted when you're trying to clean them.\n",
    "\n",
    "# Let's import and see what the data looks like\n",
    "#df = pd.read_csv('census.csv')\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/calovids/IMPRS-Python-Workshop/main/data/census.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaa1173",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(df['SUMLEV'].unique())\n",
    "#Keeping only the information that is not summarized by state\n",
    "df=df[df['SUMLEV'] == 50]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c99adc",
   "metadata": {},
   "source": [
    "### Dual indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c90a634",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The US Census data breaks down population estimates by state and county. We can load the data and \n",
    "# set the index to be a combination of the state and county values and see how pandas handles it in \n",
    "# a DataFrame. We do this by creating a list of the column identifiers we want to have indexed. And then \n",
    "# calling set index with this list and assigning the output as appropriate. We see here that we have \n",
    "# a dual index, first the state name and second the county name.\n",
    "\n",
    "df_state = df.set_index(['STNAME', 'CTYNAME'])\n",
    "df_state.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae41f66e",
   "metadata": {},
   "source": [
    "### Masks and conditional filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15274975",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also create conditional masks according to the data\n",
    "admit_mask=df_state['ESTIMATESBASE2010'] > 28000\n",
    "admit_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139c414c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, what do you do with the boolean mask once you have formed it? Well, you can just lay it on top of the\n",
    "# data to \"hide\" the data you don't want, which is represented by all of the False values. We do this by using\n",
    "# the .where() function on the original DataFrame.\n",
    "df_state.where(admit_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaa9c31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#But that looks very messy, we can drop all the NaN entries:\n",
    "df_state.where(admit_mask).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce32c2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Or, much more commonly used:\n",
    "df_state[admit_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7659a4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Also very common, but more messy\n",
    "df_state[df_state['ESTIMATESBASE2010'] > 28000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c236af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multiple conditions can quickly get out of hand for readability, also not different way to express conditions\n",
    "df_state[ (df_state['STATE'] > 50) & (df_state['ESTIMATESBASE2010']>28000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74acfa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#One could notice something weird about the values of the state variable, we can quickly investigate it\n",
    "print(len(df_state['STATE'].unique()))\n",
    "df_state['STATE'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20d212c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7fde34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#But attention, this will fail, why?\n",
    "#df_state[['STNAME','STATE']].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba546ff-a206-4136-a350-5d0ec364a7b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9317d212-4a74-41d3-a800-2590c2da359d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#The variables we wanted are now indexes,\n",
    "df[['STNAME','STATE']].drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe317632-dfe2-4990-b418-81808f6f4bed",
   "metadata": {},
   "source": [
    "### Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39c963d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mask=df_state['ESTIMATESBASE2010'].gt(28000) &df_state['ESTIMATESBASE2010'].lt(100000)\n",
    "df_state[Mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fbfe3d-a712-43ab-9053-dfe68e5dad47",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Group level measures - Groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8239540d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_state[['ESTIMATESBASE2010','POPESTIMATE2011']].groupby(level='STNAME').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e8e9ca-9453-41a6-bf48-4a62b8ed3438",
   "metadata": {},
   "source": [
    "### Calculating new metrics - apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820bbb86-f698-4f47-9df0-06099ad502d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sample data\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'Math': [92, 78, 85],\n",
    "    'Science': [88, 82, 90],\n",
    "    'English': [72, 75, 78]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4831bc66-9ca3-4b58-9990-aeda3899ac9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This DataFrame contains scores for three subjects: Math, Science, and English.\n",
    "#Now, let's say the Performance Score is calculated as the average of the\n",
    "#three subjects, but with double weightage given to Math.\n",
    "\n",
    "def calculate_performance_score(row):\n",
    "    return (2 * row['Math'] + row['Science'] + row['English']) / 4\n",
    "\n",
    "df['Performance Score'] = df.apply(calculate_performance_score, axis=1)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb6c698-6beb-4dbc-904c-3e0ff0f65d95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#While apply() is powerful, it's often slower than using vectorized operations.\n",
    "#Use it when vectorized operations are not available, but always look for a\n",
    "#vectorized solution first\n",
    "\n",
    "# Calculating Performance Score using vectorized operations\n",
    "df['Performance Score Vectorized'] = (2 * df['Math'] + df['Science'] + df['English']) / 4\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064af6fc-ff72-42dd-b7bb-a35d27225f47",
   "metadata": {},
   "source": [
    "### Question: What are some common methods used for inspecting or summarizing the contents of a DataFrame?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cd5292-e9c6-4a54-ab81-55eff805a888",
   "metadata": {},
   "source": [
    "### Question: Can you explain how to select a specific column or row from a DataFrame? What is the difference between loc and iloc?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af2da82-963d-47ec-8e65-d726d2a7a9a3",
   "metadata": {},
   "source": [
    "## Plotting Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2a7122",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_state[['ESTIMATESBASE2010']].groupby(level='STNAME').plot.box()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf5f323-7fee-4560-94e4-6c3b1825f9e7",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71126f89-5360-4728-a491-68d2a6570b74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n",
    "# List all methods and attributes of the DataFrame\n",
    "all_methods_and_attributes = dir(df)\n",
    "\n",
    "# To print or process this list, you can simply iterate over it\n",
    "for item in all_methods_and_attributes:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24bf6a8-ec3d-45b2-89bb-54184f923249",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#getting help directly in your notebook\n",
    "help(df.drop_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de773a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#useful methods\n",
    "AdmissionDF.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e766b6-e91c-4753-9fdd-18684665838b",
   "metadata": {},
   "source": [
    "# Basic statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4bc3c5-bd6c-4260-ad07-0b1fd2b8b427",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Remember our Admission dataset?\n",
    "AdmissionDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052815da-30e8-4b76-9b30-3d9d5ee46ff6",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda9c825-424a-42d5-81e2-15aa6a9f298c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using the pearson correlation function one can easily check how similar two variables are:\n",
    "# Remember how to get help in a function?\n",
    "help(AdmissionDF.corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd7889d-d29d-496a-af80-4450429bd1c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Correlation between GRE and admission probability ',AdmissionDF['GRE Score'].corr(AdmissionDF['Chance of Admit']))\n",
    "print('Correlation between GRE Score and LOR',AdmissionDF['GRE Score'].corr(AdmissionDF['Letter of Recommendation']))\n",
    "print('Correlation between University Rating and LOR',AdmissionDF['University Rating'].corr(AdmissionDF['Letter of Recommendation']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee739d3-783e-4bde-80f1-b12b67ea255c",
   "metadata": {},
   "source": [
    "## T-tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589be73d-c245-47c5-aa09-56aaee5b3359",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Generating artificial data\n",
    "group1 = np.random.normal(1, 1, 100)\n",
    "group2 = np.random.normal(1.1, 1, 100)\n",
    "t_statistic, p_value = stats.ttest_ind(group1, group2)\n",
    "print(\"T-statistic:\", t_statistic)\n",
    "print(\"p-value:\", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5e977b-a069-4533-8058-5e6103546c38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# But what happens when we have too much data\n",
    "\n",
    "group1 = np.random.normal(1, 1, 1000)\n",
    "group2 = np.random.normal(1.1, 1, 1000)\n",
    "t_statistic, p_value = stats.ttest_ind(group1, group2)\n",
    "print(\"T-statistic:\", t_statistic)\n",
    "print(\"p-value:\", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779a76dd-07d6-48cd-b226-4e33be723e99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# But what happens when we have too much data\n",
    "\n",
    "# Loop over different data sizes (100, 1000, 10000, 100000)\n",
    "for exponent in range(2, 8):\n",
    "    n = 10 ** exponent    # Generate two normally distributed datasets\n",
    "    group1 = np.random.normal(1, 1, n)\n",
    "    group2 = np.random.normal(1.001, 1, n)\n",
    "\n",
    "    # Perform an independent t-test\n",
    "    t_statistic, p_value = stats.ttest_ind(group1, group2)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Data Points: {n}\")\n",
    "    print(f\"T-statistic: {t_statistic}\")\n",
    "    print(f\"P-value: {p_value}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade3e0f6",
   "metadata": {},
   "source": [
    "# Final challenge - Mean square displacement, random walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a5fb52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##Fill out the 4 XXX_Your_Answer_Here_XXX fields\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_walkers(n_walkers, diffusion_coefficient, n_steps):\n",
    "    \"\"\"\n",
    "    Generate random walkers.\n",
    "\n",
    "    Args:\n",
    "    n_walkers (int): Number of random walkers.\n",
    "    diffusion_coefficient (float): Diffusion coefficient for the random walk.\n",
    "    n_steps (int): Number of steps for each walker.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Pandas DataFrame containing the walker ID, step number, and x, y positions.\n",
    "    \"\"\"\n",
    "    # Generate random steps for each walker\n",
    "    steps = np.random.normal(loc=0, scale=np.sqrt(2 * diffusion_coefficient), size=(n_walkers, n_steps, 2))\n",
    "    paths = np.cumsum(steps, axis=1)  # Cumulative sum to get the path\n",
    "\n",
    "    # Creating a multi-index using the product of the number of walkers and the number of steps.\n",
    "    # This creates a hierarchical index (WalkerID, Step) for the DataFrame.\n",
    "    multi_index = pd.MultiIndex.from_product([range(n_walkers), range(n_steps)], names=['XXX_Your_Answer_Here_XXX', 'XXX_Your_Answer_Here_XXX'])\n",
    "\n",
    "    # Reshape the paths array to a 2D array suitable for a DataFrame\n",
    "    # and create the DataFrame with the multi-index.\n",
    "    df = pd.DataFrame(paths.reshape(-1, 2), index=multi_index, columns=['X', 'Y'])\n",
    "\n",
    "    return df\n",
    "\n",
    "def calculate_msd_by_t(df, n_steps):\n",
    "    \"\"\"\n",
    "    Calculate the mean square displacement by time step.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): DataFrame containing walker data.\n",
    "    n_steps (int): Number of steps.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame with mean square displacement for each time step.\n",
    "    \"\"\"\n",
    "    # Use transform('first') to align each walker's start position to the origin (0,0).\n",
    "    # This subtracts the initial position of each walker from all its positions.\n",
    "    displacement_sq = (df - df.groupby(level='XXX_Your_Answer_Here_XXX').transform('first'))**2\n",
    "\n",
    "    # Sum the squared displacements in x and y, then calculate the mean for each time step.\n",
    "    msd_by_t = displacement_sq.sum(axis=1).groupby('XXX_Your_Answer_Here_XXX').mean()\n",
    "\n",
    "    return msd_by_t\n",
    "\n",
    "def plot_msd_by_t(msds, title):\n",
    "    \"\"\"\n",
    "    Plot the mean square displacement by time step in a log-log plot.\n",
    "\n",
    "    Args:\n",
    "    msds (list of DataFrames): List of DataFrames containing MSD for different diffusion coefficients.\n",
    "    title (str): Title for the plot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for msd, label in msds:\n",
    "        plt.loglog(msd.index, msd, marker='o', linestyle='-', label=f'Diffusion Coefficient: {label}')\n",
    "    plt.xlabel('Time step (\\u0394t)')\n",
    "    plt.ylabel('Mean Square Displacement')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "diffusion_coefficients = [0.1, 0.5, 1.0, 2.0]\n",
    "n_walkers = 100\n",
    "n_steps = 100\n",
    "\n",
    "msds_by_t = []\n",
    "for d in diffusion_coefficients:\n",
    "    df = generate_walkers(n_walkers, d, n_steps)\n",
    "    msd_by_t = calculate_msd_by_t(df, n_steps)\n",
    "    msds_by_t.append((msd_by_t, d))\n",
    "\n",
    "plot_msd_by_t(msds_by_t, 'Mean Square Displacement by Time Step (Log-Log Plot)')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
